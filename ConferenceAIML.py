# -*- coding: utf-8 -*-
"""Automated_Waste_Sorting_Enhancing_Recycling_Efficiency_for_Sustainable_Development.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aLBRRxr8_iGTIpdO2H1YuzKuqeOMZW1h

#*Automated Waste Sorting Enhancing Recycling Efficiency for Sustainable Development*
"""

!pip install -q kaggle

!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle

! kaggle datasets download -d saumyamohandas/garbage-classification-image-dataset

!unzip /content/garbage-classification-image-dataset.zip

import tensorflow as tf

"""# Importing libraries

"""

from keras.models import Sequential
from keras.layers import Convolution2D,Flatten,Dense,MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator

"""## Data Augmentation

# Loading Images
"""

train_datagen=ImageDataGenerator(horizontal_flip=True,rescale=1./255,zoom_range=0.2)
#rescale=1./255 means transform every pixel value from range [0,255] -> [0,1].

test_datagen=ImageDataGenerator(rescale=1./255)

X_train=train_datagen.flow_from_directory("/content/dataset/Training",
                                          target_size=(128,128),
                                          batch_size=4)

X_test=test_datagen.flow_from_directory("/content/dataset/Testing",
                                        target_size=(128,128),
                                        class_mode='categorical',
                                        batch_size=4)

X_train.class_indices

"""# ModelBuilding"""

model=Sequential()

#1)convolution layer
model.add(Convolution2D(32,(3,3),input_shape=(128,128,3),activation='relu'))

#1)maxpooling layer
model.add(MaxPooling2D(pool_size=(2,2)))

#2)convolution layer
model.add(Convolution2D(32,(3,3),activation='relu'))

#2)maxpooling layer
model.add(MaxPooling2D(pool_size=(2,2)))

#Flatten layer
model.add(Flatten())

model.summary()

model.add(Dense(300,activation='relu'))#hidden layer
model.add(Dense(150,activation='relu'))#hidden layer
model.add(Dense(6,activation='softmax'))#output layer

model.compile(optimizer="adam",loss="categorical_crossentropy",metrics=['accuracy'])

# training the model

model.fit_generator(X_train,
                    steps_per_epoch= len(X_train),
                    epochs = 30,
                    validation_data = X_test,
                    validation_steps = len(X_test))

model.save('garbage.h5')



"""# Testing"""

import numpy as np
from keras.preprocessing import image

# testing 1

img = image.load_img('/content/dataset/Testing/plastic/plastic266.jpg',target_size =(128,128))
img

x = image.img_to_array(img)
x = np.expand_dims(x,axis = 0)
pred =np.argmax(model.predict(x))
op =['cardboard','glass','metal','paper','trash','plastic']
op[pred]

# testing 2

img = image.load_img('/content/dataset/Testing/paper/paper481.jpg',target_size =(128,128))
img

x = image.img_to_array(img)
x = np.expand_dims(x,axis = 0)
pred =np.argmax(model.predict(x))
op =['cardboard','glass','metal','paper','trash','plastic']
op[pred]

# testing 3

img = image.load_img('/content/dataset/Testing/glass/glass400.jpg',target_size =(128,128))
img

x = image.img_to_array(img)
x = np.expand_dims(x,axis = 0)
pred =np.argmax(model.predict(x))
op =['cardboard','glass','metal','paper','trash','plastic']
op[pred]

# testing 4

img = image.load_img('/content/dataset/Training/trash/trash100.jpg',target_size =(128,128))
img

x = image.img_to_array(img)
x = np.expand_dims(x,axis = 0)
pred =np.argmax(model.predict(x))
op =['cardboard','glass','metal','paper','trash','plastic']
op[pred]

# testing 5

img = image.load_img('/content/dataset/Testing/metal/metal20.jpg',target_size =(128,128))
img

x = image.img_to_array(img)
x = np.expand_dims(x,axis = 0)
pred =np.argmax(model.predict(x))
op =['cardboard','glass','metal','paper','trash','plastic']
op[pred]

